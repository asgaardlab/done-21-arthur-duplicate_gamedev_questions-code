{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd4a852a-ae74-4315-947e-4babef41e419",
   "metadata": {},
   "source": [
    "# Create embeddings for questions using MPNet and BERTOverflow on Google Colab\n",
    "\n",
    "Google colab offers GPUs for free (although with a few restrictions). For small batches of texts the free tier is more than necessary.\n",
    "\n",
    "To use the GPUs provided by Google Colab, you have to change the runtime type by going to `Runtime > Change runtime type`.\n",
    "\n",
    "This notebook is meant to be used with Google Drive. To use it, copy the corpus files for each dataset into Google Drive using the same directory structure (i.e., put the files in directories like: `data/{dataset_name}/corpus/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade24dbf-4a07-4199-8c04-2fb4429ca0f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-13T21:55:27.049570Z",
     "iopub.status.busy": "2021-08-13T21:55:27.049038Z",
     "iopub.status.idle": "2021-08-13T21:55:28.718735Z",
     "shell.execute_reply": "2021-08-13T21:55:28.718210Z",
     "shell.execute_reply.started": "2021-08-13T21:55:27.049430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence_transformers in /home/athk/.local/lib/python3.9/site-packages (1.2.0)\n",
      "Requirement already satisfied: tqdm in /home/athk/.local/lib/python3.9/site-packages (from sentence_transformers) (4.60.0)\n",
      "Requirement already satisfied: torchvision in /home/athk/.local/lib/python3.9/site-packages (from sentence_transformers) (0.10.0)\n",
      "Requirement already satisfied: scipy in /home/athk/.local/lib/python3.9/site-packages (from sentence_transformers) (1.7.0)\n",
      "Requirement already satisfied: sentencepiece in /home/athk/.local/lib/python3.9/site-packages (from sentence_transformers) (0.1.95)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3.9/site-packages (from sentence_transformers) (1.20.3)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/athk/.local/lib/python3.9/site-packages (from sentence_transformers) (1.9.0)\n",
      "Requirement already satisfied: nltk in /home/athk/.local/lib/python3.9/site-packages (from sentence_transformers) (3.6.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=3.1.0 in /home/athk/.local/lib/python3.9/site-packages (from sentence_transformers) (4.7.0)\n",
      "Requirement already satisfied: scikit-learn in /home/athk/.local/lib/python3.9/site-packages (from sentence_transformers) (0.24.0)\n",
      "Requirement already satisfied: typing-extensions in /home/athk/.local/lib/python3.9/site-packages (from torch>=1.6.0->sentence_transformers) (3.7.4.3)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/athk/.local/lib/python3.9/site-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (0.10.3)\n",
      "Requirement already satisfied: requests in /usr/lib/python3.9/site-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (2.26.0)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3.9/site-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (5.4.1)\n",
      "Requirement already satisfied: sacremoses in /home/athk/.local/lib/python3.9/site-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (0.0.45)\n",
      "Requirement already satisfied: filelock in /home/athk/.local/lib/python3.9/site-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/athk/.local/lib/python3.9/site-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (2021.4.4)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /home/athk/.local/lib/python3.9/site-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (0.0.8)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3.9/site-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (20.9)\n",
      "Requirement already satisfied: joblib in /home/athk/.local/lib/python3.9/site-packages (from nltk->sentence_transformers) (1.0.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3.9/site-packages (from nltk->sentence_transformers) (8.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/lib/python3.9/site-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence_transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet>=3.0.2 in /usr/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (4.0.0)\n",
      "Requirement already satisfied: idna>=2.5 in /usr/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (3.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (1.26.6)\n",
      "Requirement already satisfied: six in /home/athk/.local/lib/python3.9/site-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence_transformers) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/athk/.local/lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /usr/lib/python3.9/site-packages (from torchvision->sentence_transformers) (8.3.1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aafedffa50f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mget_mpnet_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0mget_bertoverflow_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_cols' is not defined"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.sparse import save_npz, csr_matrix\n",
    "from pathlib import Path\n",
    "\n",
    "# Change the root folder where you put your data\n",
    "# For example, if you copied the data/ directory\n",
    "# to the root folder of your drive, the value should be `drive/MyDrive/data`\n",
    "path_to_data = 'drive/MyDrive/data'\n",
    "\n",
    "read = pd.read_parquet\n",
    "\n",
    "def make_dir(path):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "def website_dir_path(ds):\n",
    "    return Path(path_to_data) / ds\n",
    "\n",
    "def corpus_dir_path(ds):\n",
    "    return website_dir_path(ds) / 'corpus'\n",
    "\n",
    "def embeddings_dir_path(ds):\n",
    "    return website_dir_path(ds) / 'embeddings'\n",
    "\n",
    "def corpus_path(ds, tokenized=True):\n",
    "    if tokenized:\n",
    "        return corpus_dir_path(ds) / 'corpus_tokenized.parquet'\n",
    "    else:\n",
    "        return corpus_dir_path(ds) / 'corpus.parquet'\n",
    "    \n",
    "def embedding_dir_path(ds, m):\n",
    "    return embeddings_dir_path(ds) / m\n",
    "\n",
    "def embedding_path(ds, m, c):\n",
    "    return embedding_dir_path(ds, m) / f'{c}.{m}.npz'\n",
    "\n",
    "# names of the Stack Overflow samples\n",
    "so_samples = [f\"so_samples/sample_{i}\" for i in range(5)]\n",
    "\n",
    "gamedev_datasets = [\n",
    "    \"gamedev_se\",\n",
    "    \"gamedev_so\",\n",
    "]\n",
    "\n",
    "datasets = gamedev_datasets + so_samples\n",
    "\n",
    "text_columns = [\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"title_body\",\n",
    "    \"title_body_tags\",\n",
    "    \"title_body_tags_answer\",\n",
    "]\n",
    "\n",
    "def bertoverflow_model():\n",
    "    from sentence_transformers import SentenceTransformer, models\n",
    "    bertoverflow  = models.Transformer(\"jeniya/BERTOverflow\")\n",
    "    pooling_model = models.Pooling(bertoverflow.get_word_embedding_dimension())\n",
    "    return SentenceTransformer(modules=[bertoverflow, pooling_model])\n",
    "\n",
    "def mpnet_model():\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    return SentenceTransformer('paraphrase-mpnet-base-v2')\n",
    "\n",
    "def get_bertoverflow_embeddings(ds, cols, use_gpu=True):\n",
    "    \"\"\"Computes BERTOverflow embeddings and saves them\"\"\"\n",
    "    print(\"- Computing BERTOverflow embeddings\")\n",
    "\n",
    "    feature_name = \"bertoverflow\"\n",
    "\n",
    "    corpus = read(paths.corpus(ds, tokenized=False))\n",
    "\n",
    "    model = bertoverflow_model()\n",
    "\n",
    "    make_dir(paths.embedding_dir(ds, feature_name))\n",
    "\n",
    "    device = None\n",
    "    if not use_gpu:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    for c in cols:\n",
    "        print(f\"-- Computing {c} embeddings with BERTOverflow for {ds}.\")\n",
    "        emb = model.encode(corpus[c], device=device, show_progress_bar=True)\n",
    "\n",
    "        emb_save_path = paths.embedding(ds, feature_name, c)\n",
    "        save_npz(emb_save_path, csr_matrix(emb))\n",
    "\n",
    "\n",
    "def get_mpnet_embeddings(ds, cols, use_gpu=True):\n",
    "    \"\"\"Computes MPNet embeddings and saves them\"\"\"\n",
    "    print(\"- Computing MPNet embeddings\")\n",
    "    feature_name = \"mpnet\"\n",
    "\n",
    "    corpus = read(corpus_path(ds, tokenized=False))\n",
    "\n",
    "    model = mpnet_model()\n",
    "\n",
    "    make_dir(embedding_dir_path(ds, feature_name))\n",
    "\n",
    "    device = None\n",
    "    if not use_gpu:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    for c in cols:\n",
    "        print(f\"-- Computing {c} embeddings with MPNet for {ds}.\")\n",
    "        emb = model.encode(corpus[c], device=device, show_progress_bar=True)\n",
    "\n",
    "        emb_save_path = embedding_path(ds, feature_name, c)\n",
    "        save_npz(emb_save_path, csr_matrix(emb))\n",
    "\n",
    "for ds in datasets:\n",
    "    get_mpnet_embeddings(ds, text_columns)\n",
    "    get_bertoverflow_embeddings(ds, text_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
